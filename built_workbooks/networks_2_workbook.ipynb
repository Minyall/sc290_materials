{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c23252c",
   "metadata": {},
   "source": [
    "## Structure\n",
    "- Text to entities\n",
    "- Loading into networkx\n",
    "- Standard metrics and measures\n",
    "- Communities\n",
    "- Visualising with Gephi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75273db7",
   "metadata": {},
   "source": [
    "# From Text to Network\n",
    "- Last session we looked at how tv-shows and films could be transformed into relational data by thinking about them differently. \n",
    "- By representing them as characters co-occuring in scenes we were able to create a representation that showed us who the most central and critical characters were, and in some cases, demarcate out storylines through the clustering of those characters.\n",
    "- If you knew the franchise or series, the results would not have been a revelation, and this is a good thing.\n",
    "- Even if you'd never heard of 'Friends' if you were shown that network you would know who the six main characters were. You would perhaps know who the most prominent side characters were, and so forth. \n",
    "- Even if you knew the series well, it may show you patterns you didn't necessarily realise by consuming it directly.\n",
    "\n",
    "Today we're going to build a dataset for a different series, the news. We'll identify the main characters, the side characters, the storylines and perhaps see things we wouldn't see otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d6d516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the nlp model\n",
    "\n",
    "# Load in our dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6bfa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample of the first 100 articles - .copy means any changes we make are completely seperarate to our original dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f33ce",
   "metadata": {},
   "source": [
    "## Creating our 'Scenes'\n",
    "In our TV/film data, the material had been subdivided up into scenes so we could then ask \"how often were these characters 'together'\". In our data we have articles, which are analogous to 'episodes', and then paragraphs, which are analogous to scenes, subdivisions of the whole. If we think about writing, if two names are mentioned in a single paragraph it is probably because something relates them together. Before we hunt for names, let's subdivide our data into paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9832f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to make a column of paragraphs by splitting the cleaned text wherever there is a newline character \\n\n",
    "\n",
    "# We can see what the first row of this column looks like as an example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c987bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we're going to make it so that every row is a single paragraph. The index will keep track of which article each paragraph relates to.\n",
    "\n",
    "# and then we'll take just the paragraphs column, give it a new index and keep the old one, calling it 'article_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306a6d9b",
   "metadata": {},
   "source": [
    "## Getting our 'characters'\n",
    "We are now going to check each paragraph in each story for the presence of person and organisation names. We refer to these as 'entities' and the process of finding them as... \n",
    "\n",
    "\n",
    "### Named Entity Recognition\n",
    "A technique used in natural language processing to identify specific types of information in text, such as names, organisations, places, dates, etc. In the good old days, entities were identified by matching against great big lists of notable names, places etc. It was developed primarily to improve the ability of computers to answer questions more directly for things like search and building knowledge bases. \n",
    "\n",
    "Today transformer based text models like Spacy identify entities based on analysis of the text itself, based on the linguistic relationships between the words used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec172d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of different types of entities in a piece of text.\n",
    "\n",
    "# Source: https://www.bbc.co.uk/news/articles/cdx2rk10ep0o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fe8bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To extract the entities in a spacy processed document we access the .ents method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eed1c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and each entity has its own attributes, .text and .label_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1304ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply this to our paragraphs\n",
    "\n",
    "# we define the entity types we want to keep - \n",
    "# (this could be a list but generally if you don't intend to change that list it is clearer to use a tuple)\n",
    "\n",
    "# Our sample is only 100 items big but if you want to do larger amounts it's good to manage the pipe batch size.\n",
    "#  Refer back to session 2 for more detail.\n",
    "\n",
    "# Our destination list. Each item will be a list of entities in a paragraph\n",
    "\n",
    "# For every paragraph in our paragraph_data['paragraph'] colum, processed in a spacy pipe using 1 process, handling 150 items at a time\n",
    "\n",
    "# assert checks if a statement is true and throws an error if it is False\n",
    "# Here we assert that our destination list is the same length as our number of paragraphs\n",
    "\n",
    "# We then take that list and turn it into a column in our dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whilst it seemed a lot, without the comments it is quite a concise process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b2a3ba",
   "metadata": {},
   "source": [
    "## From Entities to Network\n",
    "We now have our list of paragraphs (scenes) and entities (characters). Finally we're going to make a network.\n",
    "- A Node: Represents an entity in the paragraphs.\n",
    "- An Edge: Represents a co-occurence of two entities.\n",
    "\n",
    "Node Attributes\n",
    "- `n_paragraphs`: Number of paragraphs the entity occurs in.\n",
    "- `n_articles`: Number of articles the entity occurs in.\n",
    "\n",
    "Edge Attributes\n",
    "- `'weight'`: Number of times the two entities co-occur in a paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aeb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We transform our data to be an entity per row. Again the index tracks which entities were originally together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d4b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node attributes first\n",
    "## If we group the above by entities then we can count the number of rows containing that entity, that's n_paragraphs.\n",
    "# We can also count the number of unique article_idx numbers, that's n_articles\n",
    "\n",
    "# Finally we transform this into a dictionary, ensuring that the key is the entity name (orient='index')\n",
    "# We'll use this dictionary of attributes later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ad1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we're going to create an adjacency matrix. \n",
    "# This shows the structure of the network and takes care of the edge weight attribute\n",
    "\n",
    "# First we turn our column of individual entities into a dummy matrix\n",
    "# Each row represents an entity - for each row every column will be 0 apart from the column matching that entity's name which will be 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb4b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make it one row per paragraph, we group by the index (level=0) and add the rows together.\n",
    "# Now each row shows 1 in the column if that entity is in the paragraph, otherwise 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c68b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic time\n",
    "# This is called matrix multiplication. It is a key part of graph theory.\n",
    "# All we need to know is that it is able to calculate how many times each of the items in our columns co-occur.\n",
    "\n",
    "# An adjacency matrix has all the information needed to make a weighted network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we can make our network using networkx\n",
    "\n",
    "# We can check if the network's attributes look right using...\n",
    "# G.nodes(data=True)\n",
    "# G.edges(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221661a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's export and take a look in Gephi\n",
    "\n",
    "# For now, just ignore these lines. They are filtering the graph, but we'll explain them later.\n",
    "\n",
    "# This line exports the graph to a gexf file that is compatible with gephi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922655ce",
   "metadata": {},
   "source": [
    "# Exploring our Network\n",
    "\n",
    "<img src=\"https://github.com/Minyall/sc207_290_public/blob/main/images/gephi_settings.png?raw=true\" align=\"right\" height=200>\n",
    "\n",
    "When we first open our graph in Gephi, we'll get a mess of nodes and edges. The filters shown in the image is just one way of reducing the noise in the graph. In Gephi, filters are nested and should be read bottom to top, right to left. So reading in that order the filters shown in the picture...\n",
    "1. Retains only edges that have **edge weights** between 2 and 39. 39 will just be the largest weight we have so essentially it says, minimum weight is 2.\n",
    "2. Then that remaining graph is passed to the next filter, which says each node must have a minimum **degree** of 2, i.e. a node must be connected to at least two other nodes. \n",
    "3. Then *that* graph is passed to the final filter, **Giant Component**. Components are those clusters of connected nodes. We have one big one in the middle, and lots of little ones floating around it. The Giant component is the biggest one, so essentially it only retains nodes and edges part of that large cluster in the middle.\n",
    "\n",
    "Gephi's filter UI is a little unintuitive, but it makes sense once you understand that every filter will have an impact on the next filter applied. Removing edges that have too low an edge weight, might disconnect nodes, reducing their degree, and may disconnect them from the giant component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f13791",
   "metadata": {},
   "source": [
    "\n",
    "#### Should I always filter like this?\n",
    "No. It very much depends on your data, and what you want to know. With the attributes we gave our data we could also filter by number of paragraphs a person appeared in, or number of articles. Each measurement filters people in different ways.\n",
    "\n",
    "- Degree - Number of connections - Number of unique entities a node co-occured with.\n",
    "- Edge weight - Number of times two entities co-occured.\n",
    "- n_paragraphs / n_articles - Number of paragraphs an entity appeared in or number of articles. Someone may occur many times in just one article.\n",
    "- Giant component - Filters out anymore not connected to the largest mass, but what if you have two relatively equal disconnected masses?\n",
    "\n",
    "Each choice in your filtering, as well as the order you do the filtering, has an impact on any subsequent analysis (like community detection) and the final outcome. It is important to understand what each filter is doing in relation to *your data* and what it means for *your question*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65305774",
   "metadata": {},
   "source": [
    "## Filtering with NetworkX\n",
    "Whilst you can filter with Gephi, you may find it useful to filter with networkx for various reasons. \n",
    "- Primarily it may be that having explored your data in Gephi, you want to lock in place a certain filtered version of your data. \n",
    "    - Solidifying your filtering into clear steps in your code makes it clear exactly what filtering took place, in what order, and ensures it is consistent if you need to run analysis again.\n",
    "- You may want to use NetworkX to get exact figures on various metrics, and not want to shift over the Gephi to retrieve them.\n",
    "- You may want to integrate the outputs of network analysis into a larger project - (more on this next week).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26926971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll rebuild the graph again just so we're clear on what we're starting with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6874181c",
   "metadata": {},
   "source": [
    "The two main filtering methods are `.remove_nodes_from` and `.remove_edges_from`. Each takes either a list of nodes to remove, or a list of edges. To filter the graph in some way, what differs is how you identify the nodes and/or edges to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd41f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self loops - edges that connect nodes to themselves.\n",
    "## The way we make our graph via an adjacency matrix will always produce self-loop edges.\n",
    "## However, they should be removed for our analysis as it makes no sense to consider that Person A appears in the same paragraph as Person A.\n",
    "\n",
    "# We identify the edges to remove using this networkx function\n",
    "\n",
    "# and then remove them from our graph\n",
    "\n",
    "# this is just so we can see the changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116c43e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing based on edge weight\n",
    "## For this one we need to build our list of edges by checking each edge to see if its weight attribute meets our criteria.\n",
    "\n",
    "# and we remove the edges\n",
    "\n",
    "# Note that removing edges will not remove nodes, even if they now have zero edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee4fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing nodes by attribute\n",
    "## Like above, we can check the attributes of nodes and remove those we don't want\n",
    "### For example, filtering by n_paragraphs\n",
    "\n",
    "#  we call the appropriate method\n",
    "\n",
    "# Note that removing nodes will remove edges if they lose either their source or target node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7584ba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove nodes by degree\n",
    "## Uses the .degree method \n",
    "\n",
    "# Note we lose nodes AND edges because we will have disconnected nodes \n",
    "# if they only have one connection, so we lose the edges too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e2d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components\n",
    "## Firstly we can check how many components there are in a graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc97dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components\n",
    "## The nx.connected_components function will produce lists of the nodes in each seperate component\n",
    "\n",
    "## We have to use list() to force it to actually produce something so we can examine it, as it is a generator - look it up.\n",
    "\n",
    "## Examining one of the components we see the nodes that form it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc7772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components\n",
    "## To get the biggest one use the built in max function\n",
    "### key= tells it what to use as its criteria for determining biggest, here it is length of the list.\n",
    "\n",
    "## In this case, rather than remove nodes, we say we want a graph that contains nodes\n",
    "## from our largest component, and we want all the edges between them.\n",
    "## For this we use .subgraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e93801d",
   "metadata": {},
   "source": [
    "## Summary: Applying our Gephi Filters in NetworkX\n",
    "To recap, let's start again with a new graph, and apply the filters from our Gephi example.\n",
    "\n",
    "<img src=\"https://github.com/Minyall/sc207_290_public/blob/main/images/gephi_settings.png?raw=true\" height=200>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda547b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting again, again!\n",
    "\n",
    "# Remove self-loops - We do this here because it should always be standard for this kind of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9b25b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove low weight edges\n",
    "\n",
    "# Remove low degree nodes (this will also clean up any nodes that were disconnected by the edge filter above)\n",
    "\n",
    "# Get the giant component\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009c1d9f",
   "metadata": {},
   "source": [
    "## Community Detection\n",
    "Whilst we can do community detection in Gephi, we can also do it in NetworkX. Why? \n",
    "- Because we may want those community assignments available to us here in Python without stepping over to Gephi.\n",
    "- We may want to test different settings and see their effects.\n",
    "- It helps reinforce the understanding that a Network is not just about the visual 'picture' but that it ultimately is a representation of relational data that we can interrogate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949366af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c610a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7be63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cd5533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c4678c1",
   "metadata": {},
   "source": [
    "## Graph Metrics\n",
    "A key benefit of representing you data as a network is the different kinds of measures that you can derive from that structure.\n",
    "\n",
    "We covered the metrics themselves in prior material, but here is how to use them in Networkx. All the metrics work in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b82b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dictionary of scores for the chosen metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dccfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column for the metric and use map to match the scores to the right rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057f19d0",
   "metadata": {},
   "source": [
    "A simple model for this is below, just switch the metric name to the one you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a55c290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c4a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if you want to know the scores for nodes within a specific community instead? \n",
    "# For all the figures within a specific community, which of them is the most important?\n",
    "# The metrics calculated above consider the whole network, what if we want just a subset of the network?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbdc15e",
   "metadata": {},
   "source": [
    "# Egos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e1f803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c43b811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c91fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f97832c1",
   "metadata": {},
   "source": [
    "### Notes on Metrics\n",
    "#### Degree Centrality\n",
    "We know degree is the number of edges a node has. However, *Degree Centrality* is a slightly adjusted version of this that makes it easier to know if the degree is high or low. Degree centrality is simply a node's degree frequency, divided by the total number of possible connections it could have (not including itself).\n",
    "\n",
    "#### Betweenness Centrality\n",
    "Indicates the extent to which a node stands between two others. In our data this could indicate people central to the issue overall across the different stories, but it could also indicate people that connect a side issue to a larger whole. \n",
    "\n",
    "Unweighted just considers if there is an edge, weighted factors in if co-occurences happen a lot.\n",
    "#### Eigenvector Centrality and PageRank\n",
    "Indicates the importance of a node based on the importance of the nodes it is connected to. For us this could indicate our core people, but also individuals that may be important by virtue of their closeness to important people.\n",
    "#### Closeness Centrality\n",
    "How close is a node to the rest of the network? On average how many steps would it take to get from a node to any other node in the network. A high closeness centrality indicates that a node is closer to all other nodes. It is often used to indicate access to other nodes, or information flow in a network. How easy would it be for you to be introduced to any other person in the university?\n",
    "\n",
    "In our graph this indicates how connected a person is across a range of topics. If someone could easily 'step' from their position to any other person in the network in small number of steps they are likely an important figure in the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a1cd27",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc290_materials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
