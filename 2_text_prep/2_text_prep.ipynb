{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Pre-Lab Instructions\n",
    "> <img src=\"https://github.com/Minyall/sc207_290_public/blob/main/images/attention.webp?raw=true\" align=\"right\" height=150>\n",
    "> This poor man will be at the start of every notebook, letting you know what you will need for the lab.\n",
    "> \n",
    "> For this lab you will need:\n",
    "> - DATA: `farright_dataset.parquet` - Download from Moodle and upload to this Colab session.\n",
    "> - IF YOU'RE *NOT* USING COLAB - You will need to install `spacy` and `beautifulsoup4` and the spacy model, use the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# If you are NOT using Google Colab you'll need to uncomment the lines below and run this cell to install spacy and its model\n",
    "# import sys\n",
    "# ! pip install spacy beautifulsoup4\n",
    "# !{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SC290: How to Clean and Care for your Text\n",
    "\n",
    "<img src=\"https://github.com/Minyall/sc207_290_public/blob/main/images/washing.png?raw=true\" align=\"right\" height=200>\n",
    "\n",
    "Welcome back to coding for social science research! You're here again so presumably you enjoyed your introduction in SC207, and you're ready to apply those skills to data analysis techniques that will make you look like a wizard.\n",
    "\n",
    "In today's lab we will cover:\n",
    "\n",
    "**Cleaning text**\n",
    "\n",
    "Real world text is messy, filled with symbols and stuff you don't actually want. We'll look at how to isolate the text you want and clean away the stuff you don't.\n",
    "\n",
    "**Tokenising text**\n",
    "\n",
    "Most text analysis techniques rely on breaking up long strings into small chunks, such as into individual words or 'tokens'. We'll look at one way of tokenising text to prep it ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cleaning Text\n",
    "Text cleaning is the process of removing parts of a text to only leave the 'content' that matters to you for your particular use-case. Text collected from APIs, scraped from websites, or found in existing datasets is often messy, can contain weird symbols/characters, may include special symbols that help with the formatting and display of the text when it is on a website etc.\n",
    "\n",
    "Cleaning texts is very source specific, meaning exactly what you need to do to text can vary a lot depending on where it came from, and your particular needs.\n",
    "\n",
    "Today in cleaning we're going to focus on three things.\n",
    "- Using the HTML formatting in a piece of text to isolate and remove parts that are not part of the primary content.\n",
    "- Replacing specific characters that are unusual and will be misunderstood in later text processing packages.\n",
    "- Cleaning out the HTML formatting to leave us with just plain text.\n",
    "\n",
    "> **About the Dataset**\n",
    ">\n",
    "> `farright_dataset.parquet` is a dataset of articles from The Guardian API, retrieved and prepped using the processes we used in SC207.\n",
    "> - Retrieving from the API using the simple query of `\"far-right\"` with a limit of 1,500 articles, ordered newest first.\n",
    "> - Only 'articles' from the 'News' pillar were retained.\n",
    "> - Unpacking nested data into its own columns and setting the correct data types\n",
    "> - Removing articles that were outliers such as sponsored content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import our libraries and load the dataset\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "articles = pd.read_parquet('farright_dataset.parquet')\n",
    "articles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We turn our pandas column of texts into a simpler list to make it compatible with BeautifulSoup and Spacy\n",
    "texts = articles['body'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Isolating and removing irrelevant parts\n",
    "Run the cell below and take a look at an example of the text of the article sent back by the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# For teaching purposes only - finds article with an <aside> element in\n",
    "idx = articles[articles['body'].str.contains('<aside')].last_valid_index()\n",
    "test_text = texts[idx]\n",
    "\n",
    "\n",
    "# Prints out the URL of the story so we can view it as it's meant to look and compare to the text we have.\n",
    "print(articles.loc[idx,'webUrl'])\n",
    "print('----')\n",
    "print(test_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About HTML\n",
    "This story is formatted using HTML, the language used to represent website content. \n",
    "\n",
    "Websites are made up of 'elements' which are defined by wrapping pieces of text between tags to show where the element begins `< >` and ends `</ >`. Whilst HTML has become more complex since its original design, generally content is wrapped in an element to control how it is displayed. \n",
    "\n",
    "For example `<p>These tags indicate content is a paragraph</p>`.\n",
    "\n",
    "Sometimes an element will also have a `class` which tells the website to format that content differently.\n",
    "```\n",
    "<p class=\"important_highlight_big\"> The most important point of our story is.... </p>\n",
    "```\n",
    "\n",
    "\n",
    "Sometimes an element can be *inside* another element \n",
    "\n",
    "```\n",
    "<p>Paragraph content is here talking about an important <aside>See our great new dog story</aside> story about cats</p>\n",
    "\n",
    "```\n",
    "### Using HTML to navigate text\n",
    "\n",
    "We simply want the text inside the most basic 'paragraph' `<p>` elements, but we also *don't* want any side content that might be embedded inside a paragraph element.\n",
    "\n",
    "To do this we will `decompose` certain elements, i.e. isolate them and cut them out of the text, before then identifying all the `<p>` elements and getting their text.\n",
    "\n",
    "The library `BeautifulSoup` is designed to read HTML text and turn it into a structured object that we can navigate, and manipulate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we take the text and use Beautiful soup to interpret it\n",
    "# We call the result 'soup', because that's what people do.\n",
    "soup = BeautifulSoup(test_text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can ask for all the 'p' paragraph elements.\n",
    "# This also will show us what other elements are embedded inside them.\n",
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are `span` and `aside` elements. We want to remove them and the text inside them.\n",
    "- There are also `<a>` elements which are how HTML indicates something is a link. The text wrapped by an `<a>` element makes that text a link, but it is also still a part of the main text, so we want to leave those in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will remove unwanted elements\n",
    "# We'll make a list of element types we want to remove\n",
    "unwanted_elements = ['span','aside']\n",
    "\n",
    "# Next we use .find_all() to iterate over every unwanted element, \n",
    "# and .decompose() it - delete it.\n",
    "for element in soup.find_all(unwanted_elements):\n",
    "    element.decompose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Comprehensions\n",
    "In this module we're going to start using something called *List Comprehensions*. These are an alternative way of looping over a list to filter or edit its contents. Rather than the usual:\n",
    "\n",
    "```\n",
    "old_list = ['a','b','c']\n",
    "new_list = []\n",
    "\n",
    "for item in old_list:\n",
    "    new_item = do_thing(item)\n",
    "    new_list.append(new_item)\n",
    "```\n",
    "\n",
    "We can instead keep it cleaner in one line: \n",
    "```\n",
    "new_list = [do_thing(item) for item in old_list]\n",
    "```\n",
    "\n",
    "The general structure of a list comprehension is:\n",
    "\n",
    "```\n",
    "[item_to_keep for item in list_to_iterate_over]\n",
    "```\n",
    "\n",
    "The job we did above could also be a list comprehension, we just don't assign the result to a variable. `.decompose()` doesn't produce any value, it just deletes an element so the list would be empty anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same decompose job as above but in one line\n",
    "[element.decompose() for element in soup.find_all(unwanted_elements)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've decomposed the elements we don't want, we just need to extract the text from every `p` element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# and we'll then retain the text associated with any p element that has no associated class\n",
    "paras = [p.text for p in soup.find_all('p')]\n",
    "paras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally we convert that list of strings into a single string, \n",
    "# retaining the paragraph breaks by inserting a new line break between each paragraph.\n",
    "cleaned_item ='\\n'.join(paras)\n",
    "print(cleaned_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 A text cleaning function\n",
    "We can do this for every article in our list. First we'll build a function to do the job of cleaning, then we'll apply it to every item in the list of texts. But first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# The Guardian sometimes uses this character rather than a normal apostrophe.\n",
    "# Tiny things like this can really throw off text analysis, so we'll fix it in our function.\n",
    "annoying_character = \"’\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_guardian_text(text, remove_elements=['span','aside']):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    [e.decompose() for e in soup.find_all(remove_elements)]\n",
    "    paras = [p.text for p in soup.find_all('p')]\n",
    "    cleaned_item ='\\n'.join(paras)\n",
    "    cleaned_item.replace(\"’\", \"'\") # replacing an annoying character used in the guardian\n",
    "    return cleaned_item\n",
    "\n",
    "cleaned_texts = [clean_guardian_text(t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Saving your cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "articles['cleaned_text'] = cleaned_texts\n",
    "articles.to_parquet('farright_dataset_cleaned.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenising\n",
    "A lot of text analysis relies on making texts into 'tokens'. A simple way of thinking about this is that it splits a text into individual words.\n",
    "\n",
    "`\"Hello, how are you?\"` might become `['Hello','how','are','you']`.\n",
    "\n",
    "This is a more complicated task than you might think, because it could also become:\n",
    "- `['Hello,','how','are','you?']`\n",
    "- `['Hello', ',','how','are','you', '?']`\n",
    "\n",
    "To help us we need a library dedicated to handling text - enter [spaCy](https://spacy.io)\n",
    "\n",
    "<img src=\"https://spacy.io/images/pipeline.svg\" height=100>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(cleaned_texts[-1])\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst this may look the same, it is now a spacy `Doc` object that understands the text and can help us break it down into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* \n",
    "# This is how spacy breaks up the document\n",
    "[t for t in doc][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# # Spacy uses the context of the surrounding words and grammar to work out if the word is a noun, verb, adjective etc.\n",
    "# They call this the 'part-of-speech' or POS\n",
    "[(t, t.pos_) for t in doc][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Spacy tokens have helpful attributes...\n",
    "# Is it alphabetical (i.e not numerical or punctuation)\n",
    "[(t.text, t.is_alpha) for t in doc][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Is it punctuation? \n",
    "[(t, t.is_punct) for t in doc][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# # Is it a stop word? \n",
    "[(t, t.is_stop) for t in doc][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Stop Words?\n",
    "Stop words are typically defined as the most common words in a language. Often incredibly common words can make it harder to find patterns in text. For example the most common words in a piece of text might be 'the', 'a', 'and' etc. That doesn't tell us much about the text even though the result is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# These are the stop words for this model\n",
    "print(nlp.Defaults.stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use these token attributes to filter our text based on what type of token it is\n",
    "\n",
    "# This ensures only alphabetical tokens that aren't stop words are retained.\n",
    "[t for t in doc if t.is_alpha and not t.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allows numbers as well, but filters out space symbols like \\r and \\n and punctuation\n",
    "\n",
    "[t for t in doc if not t.is_space and not t.is_punct and not t.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Lemmatization\n",
    "\n",
    "A word's lemma is the simpler 'root' word that best represents the word's meaning. It reduces the possible range of words whilst still ensuring the words left convey the appropriate meaning.\n",
    "\n",
    "To make this clearer we can use some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Here we have essentially the same sentences, just a variation in that one uses a contraction \"don't\" rather than \"do not\".\n",
    "rabbit_1 = nlp(\"I don't like rabbits in space\")\n",
    "rabbit_2 = nlp(\"I do not like rabbits in space\")\n",
    "print( [token.lemma_ for token in rabbit_1])\n",
    "print( [token.lemma_ for token in rabbit_2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Even differing text can be brought at least closer in similarity using lemmas, reducing loving to love\n",
    "rabbit_1 = nlp(\"I'm loving these rabbits\")\n",
    "rabbit_2 = nlp(\"I love this rabbit!\")\n",
    "\n",
    "print( [token.lemma_ for token in rabbit_1])\n",
    "print( [token.lemma_ for token in rabbit_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are doing any text analysis that counts the frequency of words, relies on word similarity etc, it is usually a good idea to reduce the range of words being used so long as it can retain the same underlying semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = [t.lemma_.lower() for t in doc if not t.is_stop and t.is_alpha]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# We can very quickly get a sense of a document's content by looking at the most common tokens\n",
    "from collections import Counter\n",
    "counts = Counter(filtered_tokens)\n",
    "counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to convert your filtered tokens to text you simply join them together again\n",
    "filtered_text = \" \".join(filtered_tokens)\n",
    "filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Tokenising in bulk\n",
    "Spacy does some pretty heavy lifting so we should tokenise once, and then save the result to avoid having to rerun the process again. Spacy also has a method that speeds up tokenising on large numbers of documents called `.pipe`.\n",
    "\n",
    "`.pipe` efficiently processes large amounts of documents by:\n",
    "- Handling them in large batches controlled by the `batch_size=` argument.\n",
    "- Running multiple workers at the same time, controlled by the `n_process=` argument.\n",
    "\n",
    "Now we're getting into analysis we're going to start encountering instances where the efficiency of code matters, because the jobs we're doing are increasingly more complex and take longer to complete.\n",
    "\n",
    "### What settings do I use?\n",
    "#### Colab\n",
    "Colab takes around 4 minutes to process 500 articles. To avoid issues you should set: \n",
    "- `batch_size=150`\n",
    "- `n_process=1`\n",
    "\n",
    "#### Your own computer\n",
    "Entirely down to the hardware you use. If you have a more powerful laptop with multiple CPU cores you can increase the number of workers to 1 less than the number of cores you have. If you have a lot of RAM you can increase the batch size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Let's do a quick reset here just to clean up any issues you may have had\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "articles = pd.read_parquet('farright_dataset_cleaned.parquet')\n",
    "cleaned_texts = articles['cleaned_text'].tolist()\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# We'll use this function as our main tokeniser\n",
    "# It takes in a single spacy Doc and outputs a single string of tokens\n",
    "def tokenise_doc(doc: spacy.tokens.Doc) -> str:\n",
    "    tokens = [t.lemma_.lower() for t in doc if not t.is_stop and t.is_alpha]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "example = nlp(cleaned_texts[-1])\n",
    "tokenise_doc(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will create a 'pipe' that will efficiently transform a list of texts\n",
    "# into spacy Doc objects. \n",
    "# We will process each Doc using our function, and store the results.\n",
    "\n",
    "BATCH_SIZE = 150\n",
    "WORKERS = 1\n",
    "\n",
    "tokenised_documents = []\n",
    "\n",
    "for doc in nlp.pipe(cleaned_texts, batch_size=BATCH_SIZE, n_process=WORKERS):\n",
    "    tokenised_documents.append(tokenise_doc(doc))\n",
    "\n",
    "articles['tokens'] = tokenised_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# We can see the result and compare with the cleaned text\n",
    "\n",
    "example_row = articles.iloc[-1]\n",
    "print(example_row['cleaned_text'][:250])\n",
    "print('****')\n",
    "print(example_row['tokens'][:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# We save our new version of our dataframe with the tokens column.\n",
    "# We'll be using these in the next few sessions.\n",
    "articles.to_parquet('farright_dataset_cleaned.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Today we covered:\n",
    "- HTML code and how to use it to isolate parts of web based text.\n",
    "- How to extract the text from HTML code.\n",
    "- Tokenising, why it is tricky to do.\n",
    "- Stop words and word lemmas\n",
    "- Bulk tokenisation of large text datasets ready for analysis.\n",
    "\n",
    "Next week we will look at how tokens can be used to represent groups of documents and help us determine document similarity and difference. These are the foundations of text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc290_materials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
