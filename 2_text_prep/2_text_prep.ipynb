{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Pre-Lab Instructions\n",
    "> <img src=\"https://github.com/Minyall/sc207_290_public/blob/main/images/attention.webp?raw=true\" height=200>\n",
    "\n",
    "> For this lab you will need:\n",
    "> - DATA: `farright_dataset.parquet` - Download from Moodle and upload to this Colab session.\n",
    "> - IF YOU'RE *NOT* USING COLAB - You will need to install `spacy` and `beautifulsoup4` and the spacy model, use the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# If you are NOT using Google Colab you'll need to uncomment the lines below and run this cell to install spacy and its model\n",
    "# import sys\n",
    "# ! pip install spacy beautifulsoup4\n",
    "# !{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying it to Guardian Data\n",
    "\n",
    "`farright_dataset.parquet` is a dataset of articles from The Guardian API, retrieved and prepped using the processes we used in SC207.\n",
    "- Retrieving from the API using the simple query of `\"far-right\"` with a limit of 1,500 articles, ordered newest first.\n",
    "- Only 'articles' from the 'News' pillar were retained.\n",
    "- Unpacking nested data into its own columns and setting the correct data types\n",
    "- Removing articles that were outliers such as sponsored content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "articles = pd.read_parquet('farright_dataset.parquet')\n",
    "articles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# We turn our pandas column of texts into a simpler list to make it compatible with BeautifulSoup and Spacy\n",
    "texts = articles['body'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# For teaching purposes only - finds first article with an <aside> element in\n",
    "idx = articles[articles['body'].str.contains('<aside')].first_valid_index()\n",
    "test_text = texts[idx]\n",
    "\n",
    "\n",
    "# Prints out the URL of the story so we can view it as it's meant to look and compare to the text we have.\n",
    "print(articles.loc[idx,'webUrl'])\n",
    "print('----')\n",
    "print(test_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a text contains more complex elements these will be wrapped in different tags that help lay it out on the website, change it's formatting etc. We simply want the text inside the most basic 'paragraph' `<p>` elements. There may even be `<p>` elements that do extra things. These will have an associated `class` which tells the website to format it differently.\n",
    "\n",
    "Sometimes there will be other elements *inside* `p` elements, such as sidebar related stories. Generally these are wrapped in `span` or `aside` tags. We will manually `decompose` these from the text - i.e. cut them out, before then identifying all the `p` elements and getting their text.\n",
    "\n",
    "Generally for text analysis we want the content text rather than headings, web addresses, embedded side content etc. Every website will differ in the best way to extract this material. Though there are general standards of tagging HTML elements it is usually necessary to customise what elements you decompose, what you keep and in what order to maximise the content you want to retain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll remove span and aside elements\n",
    "soup = BeautifulSoup(test_text, 'html.parser')\n",
    "\n",
    "remove_elements = ('span','aside')\n",
    "[e.decompose() for e in soup.find_all() if e.name in remove_elements]\n",
    "\n",
    "# and we'll then retain the text associated with any p element that has no associated class\n",
    "paras = [p.text for p in soup.find_all('p', class_=None)]\n",
    "cleaned_item ='\\n'.join(paras)\n",
    "print(cleaned_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this for every article in our list. First we'll build a function to do the job of cleaning, then we'll apply it to every item in the list of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_guardian_text(text, remove_elements=('span','aside')):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    [e.decompose() for e in soup.find_all() if e.name in remove_elements]\n",
    "    paras = [p.text for p in soup.find_all('p', class_=None)]\n",
    "    cleaned_item ='\\n'.join(paras)\n",
    "    cleaned_item.replace(\"â€™\", \"'\") # replacing an annoying character used in the guardian\n",
    "    return cleaned_item\n",
    "\n",
    "cleaned_texts = [clean_guardian_text(t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['cleaned_text'] = cleaned_texts\n",
    "articles.to_parquet('farright_dataset_cleaned.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(cleaned_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy can tell us how many 'tokens' are in the document - i.e. how many words (but also other things)\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many sentences in the document?\n",
    "len(list(doc.sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Tokens are units of text in natural language processing. Exactly how a text is 'tokenised' varies depending on the tool\n",
    "# and many debates are had about the best way to do it.\n",
    "\n",
    "# The goal is to render a text down into individual units of information that can be processed by different analysis techniques\n",
    "\n",
    "# This is how spacy breaks up the document\n",
    "[w.text for w in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#*# Spacy uses the context of the surrounding words and grammar to work out if the word is a noun, verb, adjective etc.\n",
    "# They call this the 'part-of-speech' or POS\n",
    "[(w.text, w.pos_) for w in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Spacy tokens have helpful attributes...\n",
    "# Is it alphabetical (i.e not numerical or punctuation)\n",
    "[(w.text, w.is_alpha) for w in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Is it punctuation? \n",
    "[(w.text, w.is_punct) for w in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# # Is it a stop word? \n",
    "[(w.text, w.is_stop) for w in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words?\n",
    "Stop words are typically defined as the most common words in a language. Often incredibly common words can make it harder to find patterns in text. For example the most common words in a piece of text might be 'the', 'a', 'and' etc. That doesn't tell us much about the text even though the result is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# These are the stop words for this model\n",
    "print(nlp.Defaults.stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use these token attributes to filter our text based on what type of token it is\n",
    "\n",
    "# This ensures only alphabetical tokens that aren't stop words are retained.\n",
    "[w.text for w in doc if w.is_alpha and not w.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allows numbers as well, but filters out space symbols like \\r and \\n and punctuation\n",
    "\n",
    "[w.text for w in doc if not w.is_space and not w.is_punct and not w.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "A word's lemma is the simpler 'root' word that best represents the word's meaning. It reduces the possible range of words whilst still ensuring the words left convey the appropriate meaning.\n",
    "\n",
    "To make this clearer we can use some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Here we have essentially the same sentences, just a variation in that one uses a contraction \"don't\" rather than \"do not\".\n",
    "rabbit_1 = nlp(\"I don't like rabbits in space\")\n",
    "rabbit_2 = nlp(\"I do not like rabbits in space\")\n",
    "print( [token.lemma_ for token in rabbit_1])\n",
    "print( [token.lemma_ for token in rabbit_2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*\n",
    "# Even differing text can be brought at least closer in similarity using lemmas, reducing loving to love\n",
    "rabbit_1 = nlp(\"I'm loving these rabbits\")\n",
    "rabbit_2 = nlp(\"I love this rabbit!\")\n",
    "\n",
    "print( [token.lemma_ for token in rabbit_1])\n",
    "print( [token.lemma_ for token in rabbit_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are doing any text analysis that counts the frequency of words, relies on word similarity etc, it is usually a good idea to reduce the range of words being used so long as it can retain the same underlying semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = [w.lemma_.lower() for w in doc if not w.is_stop and  w.is_alpha]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(filtered_tokens)\n",
    "counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to convert your filtered tokens to text you simply join them together again\n",
    "\n",
    "\n",
    "filtered_text = \" \".join(filtered_tokens)\n",
    "filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenising in bulk\n",
    "Spacy does some pretty heavy lifting so we should tokenise once, and then save the result to avoid having to rerun thr process again. Spacy also has a method that speeds up tokenising on large numbers of documents. Now we're getting into analysis we're going to start encountering the actual nuts and bolts of using a computer because the size of our datasets and the complexity of what we're doing can put a real strain on the actual hardware used.\n",
    "\n",
    "Depending on what kind of computer we have available we may have to tweak different settings to avoid analysis failing or hardware crashing. Often the things we have to balance are...\n",
    "- How much information can the computer keep in its memory at one time (RAM) controlled by `batch_size=`\n",
    "- How many workers can run at the same time (CPUS) controlled by `n_process=`\n",
    "- How long are things going to take to finish (Your patience) controlled by `how_close_the_deadline_is=`<sup>*</sup>\n",
    "\n",
    "Spacy's `.pipe` method can help us here. It can take a stack of texts and we can tell it how many workers to start running and how many texts each worker should handle at a time.\n",
    "\n",
    " Generally if you're using Google Colab it takes around 4 minutes to process 500 articles. To avoid the hardware being overloaded and failing to finish you should set the batch_size to be between 150 and 200 and leave it using just 1 worker. \n",
    " \n",
    "If you have a more powerful laptop with multiple cores you can increase the number of workers and if you have a lot of RAM you can increase the batch size.\n",
    "\n",
    "<sub>* Unfortunately not a real argument</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "articles = pd.read_parquet('farright_dataset_cleaned.parquet')\n",
    "cleaned_texts = articles['cleaned_text'].tolist()\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_doc(doc):\n",
    "    tokens = [w.lemma_.lower() for w in doc if not w.is_stop and w.is_alpha]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "BATCH_SIZE = 150\n",
    "WORKERS = 1\n",
    "\n",
    "\n",
    "tokens = []\n",
    "for doc in nlp.pipe(cleaned_texts, batch_size=BATCH_SIZE, n_process=WORKERS):\n",
    "    tokens.append(tokenise_doc(doc))\n",
    "\n",
    "articles['tokens'] = tokens\n",
    "articles.to_parquet('farright_dataset_cleaned.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for toks in tokens[:5]:\n",
    "    print(Counter(toks.split()).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc290_materials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
